{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kt-chan/Huawei-FinGPT/blob/master/unstructured_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZbrRMSiG8xF"
      },
      "source": [
        "# Install Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGiBMnQzCqk",
        "outputId": "335e6993-4901-4498-8cad-68c4160c5e7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "\r0% [Waiting for headers] [Connecting to security.ubuntu.com (91.189.91.83)] [Connected to cloud.r-pr\r                                                                                                    \rGet:2 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [128 kB]\n",
            "\r                                                                                                    \rHit:3 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "\r                                                                                                    \rGet:4 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r                                                                                                    \rHit:5 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "\r                                                                                                    \r0% [Waiting for headers] [Waiting for headers]\r                                              \rGet:6 http://security.ubuntu.com/ubuntu jammy-security InRelease [129 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,394 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [2,233 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [2,632 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [2,559 kB]\n",
            "Get:15 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,969 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu jammy-security/multiverse amd64 Packages [44.7 kB]\n",
            "Get:17 http://security.ubuntu.com/ubuntu jammy-security/universe amd64 Packages [1,112 kB]\n",
            "Fetched 12.2 MB in 7s (1,808 kB/s)\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra libreoffice libtesseract-dev libmagic-dev  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "collapsed": true,
        "id": "IzBi-ChGvCXV"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured unstructured-inference unstructured_pytesseract langchain openai chromadb pillow_heif pytesseract treelib  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeUSDVtHEhP"
      },
      "source": [
        "# Content Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9JE0kv8-zW"
      },
      "source": [
        "Parittion the pdf files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQXeITIH60Dv",
        "outputId": "d0c29a17-17d4-4547-ee53-7f1586801c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./files’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ./files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOttQcEva_m",
        "outputId": "bb88172b-96f1-4055-df83-8232d36bec2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed. File saved as: /content/files/downloaded_file.pdf\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "\n",
        "# For this notebook I uploaded Nvidia's earnings into the Files directory called \"/content/\"\n",
        "pdf_url = \"https://static.www.tencent.com/uploads/2024/04/08/e95c902973fc282be3b3e285c6245281.pdf\"\n",
        "output_dir = \"./files/\"\n",
        "\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    full_path = os.path.abspath(output_dir+filename)\n",
        "    with open(full_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f'Download completed. File saved as: {full_path}')\n",
        "    return full_path\n",
        "\n",
        "\n",
        "filename = download_pdf(pdf_url, 'downloaded_file.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB16l73cxbEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unstructured.staging.base import elements_to_text\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import group_broken_paragraphs\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "# Partition PDF\n",
        "# Define parameters for Unstructured's library\n",
        "strategy = \"hi_res\" # Strategy for analyzing PDFs and extracting table structure\n",
        "model_name = \"yolox\" # Best model for table extraction. Other options are detectron2_onnx and chipper depending on file layout\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "  filename=filename,\n",
        "  strategy=strategy,\n",
        "  infer_table_structure=True,\n",
        "  model_name=model_name\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Extract tables\n",
        "para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")\n",
        "element_text = elements_to_text(elements)\n",
        "element_text = group_broken_paragraphs(element_text, paragraph_split=para_split_re)\n",
        "element_text = clean_non_ascii_chars(element_text)\n",
        "\n",
        "# Store results in files\n",
        "elements_to_json(elements, filename=f\"{filename}.json\") # Takes a while for file to show up on the Google Colab\n",
        "elements_to_text(elements, filename=f\"{filename}.text\") # Takes a while for file to show up on the Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkrTCcgiEqxh"
      },
      "source": [
        "# Setup MongoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2zVG69g45wm"
      },
      "source": [
        "Connection Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT3b_EqlE3Dl",
        "outputId": "45f6abec-42b3-4071-9359-e7336d4f513d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.32.183.186\n",
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h\u001b[33mWARNING: pymongo 4.8.0 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo[srv]) (2.6.1)\n",
            "Installing collected packages: pymongo\n",
            "Successfully installed pymongo-4.8.0\n"
          ]
        }
      ],
      "source": [
        "# set this to ip to MongoDB atlas firewall rules, https://cloud.mongodb.com/v2/667549db2c13183084c47650#/security/network/accessList\n",
        "!curl ifconfig.me\n",
        "print()\n",
        "!python -m pip install \"pymongo[srv]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBTTxYjiMD42",
        "outputId": "eb483d87-8d29-4433-df69-f7771c5cec7e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "user = userdata.get('mongodb_username')  ## replace with mongodb atlas username\n",
        "password = userdata.get('mongodb_password')## replace with mongodb atlas password\n",
        "uri = \"mongodb+srv://\"+user+\":\"+password+\"@ktchan-mongo-atlast.mvx53s5.mongodb.net/?retryWrites=true&w=majority&appName=ktchan-mongo-atlast\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5_Wsrc-YGp"
      },
      "source": [
        "# Table Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-Klk_s9BeC"
      },
      "source": [
        "Extract tables from paritions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mswW8i_FcwHZ",
        "outputId": "f30ba1bb-5c31-4877-d0c8-563434b2358d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written %d documents into mongodb collection_tencent. 158\n",
            "Written %d documents into mongodb collection_tencent_notes. 3476\n",
            "Element Tree size is %d. 2795\n"
          ]
        }
      ],
      "source": [
        "## In order to extract only the table elements I’ve written a helper function to do so:\n",
        "import re\n",
        "import json\n",
        "from html import escape\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from treelib import Node, Tree\n",
        "\n",
        "element_tree = Tree()\n",
        "extracted_elements = {}\n",
        "extracted_elements_table = []\n",
        "\n",
        "\n",
        "def extract_json_elements(input_filename):\n",
        "  # Read the JSON file\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required table elements\n",
        "    for entry in data:\n",
        "      text = entry[\"text\"]\n",
        "\n",
        "      if \"Table\" == entry[\"type\"]:\n",
        "          text = entry[\"metadata\"][\"text_as_html\"]\n",
        "\n",
        "      if 'parent_id' in entry['metadata']:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "          \"_id\": entry[\"element_id\"],\n",
        "          \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "          \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "          \"parent_id\":entry[\"metadata\"][\"parent_id\"],\n",
        "          \"type\":entry[\"type\"],\n",
        "          \"text\":text\n",
        "          }\n",
        "      else:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "            \"_id\": entry[\"element_id\"],\n",
        "            \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "            \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "            \"type\":entry[\"type\"],\n",
        "            \"text\":text\n",
        "            }\n",
        "\n",
        "  # construct element dependency tree\n",
        "  for k, element in extracted_elements.items():\n",
        "      if \"parent_id\" in element:\n",
        "        # print(element)\n",
        "        parent_target = element[\"parent_id\"]\n",
        "        node = element_tree.get_node(parent_target)\n",
        "        if node:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "        else:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          # element_parent = extracted_elements[parent_target].pop(\"parent_id\", None)\n",
        "          element_tree.create_node(parent_target, parent_target, parent=\"root\", data = extracted_elements[parent_target])\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "\n",
        "  return extracted_elements\n",
        "\n",
        "\n",
        "\n",
        "def extract_json_table(input_filename):\n",
        "  # Read the JSON file\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required table elements\n",
        "    for entry in data:\n",
        "      if entry[\"type\"] == \"Table\":\n",
        "        entry[\"metadata\"][\"element_id\"] = entry[\"element_id\"]\n",
        "        extracted_elements_table.append({\"element_id\":entry[\"element_id\"], \"metadata\" : entry[\"metadata\"]})\n",
        "\n",
        "  return extracted_elements_table\n",
        "\n",
        "\n",
        "#Define a function to clean table columns\n",
        "def extract_table_from_html(table_html_string):\n",
        "  # Rename the columns to be just the first element of each tuple\n",
        "  # Function to convert elements to string\n",
        "  def to_str(element):\n",
        "      if isinstance(element, tuple):\n",
        "          return '_'.join(map(str, element))  # Join tuple elements with an underscore\n",
        "      else:\n",
        "          return str(element)  # Convert integer to string\n",
        "\n",
        "  # Find the table within the span element\n",
        "  table = table_html_string.find('table')\n",
        "  df_flattened = None\n",
        "  if table:\n",
        "      table_dfs = pd.read_html(table_html_string) # Get the first DataFrame\n",
        "      table_df = table_dfs[0] # assume only one table per json tab\n",
        "      df_flattened = table_df.applymap(str)  # Convert all values to string\n",
        "      df_flattened.columns = [to_str(col) for col in df_flattened.columns] # Column headers parsed as set by pdf extractor, force to string for compatibility\n",
        "\n",
        "  return df_flattened\n",
        "\n",
        "\n",
        "# Define a fuctnion to formatn tables cells\n",
        "def format_table(df_table):\n",
        "    # df_table =  pd.DataFrame(ds_table)\n",
        "    df_numeric = df_table.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "    df_combined = pd.concat([df_table.iloc[:, 0], df_numeric], axis=1)\n",
        "    df_cleaned = df_combined.dropna(how='any')\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def clean_text(text):\n",
        "\n",
        "    # Function to convert matched text to negative number\n",
        "    def make_negative(match):\n",
        "        number = match.group(1).replace(',', '')\n",
        "        return f\"{-int(number)}\"\n",
        "\n",
        "    # Check if the text is a string\n",
        "    if isinstance(text, str):\n",
        "        # Replace unwanted characters if it's a string\n",
        "        text = text.replace('#', '').replace('*', '')\n",
        "\n",
        "\n",
        "        # Regular expression to match numbers in parentheses with commas\n",
        "        pattern = r'\\((\\d{1,3}(,\\d{3})*)\\)'\n",
        "        # Replace all occurrences of the pattern with its negative equivalent\n",
        "        text = re.sub(pattern, make_negative, text)\n",
        "\n",
        "        # Check if the text is a number with commas\n",
        "        if re.match(r'^-?\\d{1,3}(,\\d{3})*\\.\\d+$', text):\n",
        "            # Remove commas and convert to float\n",
        "            return float(text.replace(',', ''))\n",
        "        # Check if the text is an integer with commas\n",
        "        elif re.match(r'^-?\\d{1,3}(,\\d{3})*$', text):\n",
        "            # Remove commas and convert to integer\n",
        "            return int(text.replace(',', ''))\n",
        "    # Return the text as is if it's not a string\n",
        "    return text\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def parse_html_table(data_tables):\n",
        "\n",
        "  # Initialize an empty dictionary to hold the tables with span_id as the key\n",
        "  # and the table data as the value\n",
        "  tables_dict = {}\n",
        "\n",
        "\n",
        "  # Find all span elements with an 'id' attribute\n",
        "  for data_table in data_tables:\n",
        "      span_id = data_table['element_id']\n",
        "\n",
        "      # Initialize a dictionary to hold the metadata and table data for the current span\n",
        "      span_data = {}\n",
        "\n",
        "      # Extract the 'metadata' attribute if it exists\n",
        "      metadata = data_table.get('metadata', None)\n",
        "\n",
        "      if metadata:\n",
        "          span_data['metadata'] = metadata\n",
        "\n",
        "      # Find the table within the span element\n",
        "      table = data_table.get('text_as_html', None)\n",
        "      if table:\n",
        "          # Use pandas to read the table\n",
        "          table_df = pd.read_html(str(table)) # Get the first DataFrame\n",
        "          span_data['table'] = table_df\n",
        "\n",
        "      # Add the span data to the main dictionary using the span_id as the key\n",
        "      tables_dict[span_id] = span_data\n",
        "\n",
        "\n",
        "  datasets = []\n",
        "\n",
        "  # Get the first key-value pair based on insertion order\n",
        "  for span_id, span_data in tables_dict.items():\n",
        "      # Access the metadata and the table DataFrame\n",
        "      metadata = span_data.get('metadata')\n",
        "      table_html = metadata.get('text_as_html')\n",
        "      df_table = extract_table_from_html(table_html)\n",
        "      df_table_clean = df_table.applymap(clean_text)\n",
        "      datasets.append({\n",
        "          \"_id\": span_id,  # Corrected the syntax for dictionary keys (no quotes)\n",
        "          \"meta\": metadata,  # Corrected the variable name ('metadata' instead of 'metdata')\n",
        "          \"data\": df_table_clean.to_dict(\"records\"),  # Assuming you want to store the first DataFrame in the list\n",
        "          \"data_raw\": df_table.to_dict(\"records\")  # raw data format, without formatting\n",
        "          })\n",
        "\n",
        "  return datasets\n",
        "\n",
        "\n",
        "def process_extraction(filePath):\n",
        "  element_tree.create_node(\"root\", \"root\")\n",
        "  data_elements = extract_json_elements(filePath)\n",
        "  data_tables = extract_json_table(filePath)\n",
        "  return parse_html_table(data_tables)\n",
        "\n",
        "\n",
        "\n",
        "file_path=\"/content/files/downloaded_file.pdf.json\"\n",
        "db = client['db_finance_report']\n",
        "collection = db['collection_tencent']\n",
        "collection.drop()\n",
        "insert_collection = process_extraction(file_path)\n",
        "collection.insert_many(insert_collection);\n",
        "print(f'Written %d documents into mongodb collection_tencent.', collection.count_documents({}))\n",
        "\n",
        "extracted_elements_val=[]\n",
        "for k, v in extracted_elements.items():\n",
        "  extracted_elements_val.append(v)\n",
        "\n",
        "collection = db['collection_tencent_notes']\n",
        "collection.drop()\n",
        "collection.insert_many(extracted_elements_val);\n",
        "print(f'Written %d documents into mongodb collection_tencent_notes.', collection.count_documents({}))\n",
        "\n",
        "\n",
        "print(f'Element Tree size is %d.', element_tree.size())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-C5KuaBV8W"
      },
      "outputs": [],
      "source": [
        "# print(element_tree)\n",
        "\n",
        "# Depth-First Search (DFS) traversal\n",
        "def dfs_traversal(node, tree):\n",
        "    print(node)\n",
        "    for child in tree.children(node.identifier):\n",
        "        dfs_traversal(child, tree)\n",
        "\n",
        "# Start the traversal from the root node\n",
        "# target_node = element_tree.get_node(\"0ce1e2d0bb4281b12e30ad32b5653f36\")\n",
        "dfs_traversal(element_tree.get_node(\"root\"), element_tree)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nao00jRUNfdJ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-VU9tmNY8Jk-"
      },
      "source": [
        "This is for testing the maths group function on clean tables."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ksnhhstiqhPY"
      },
      "outputs": [],
      "source": [
        "db = client['db_finance_report']\n",
        "collection = db['collection_tencent']\n",
        "cursor = collection.find({\"_id\":\"7e371433d7d983a74a1a0c1c313cd482\"},{\"_id\":0, \"data\":1})\n",
        "\n",
        "\n",
        "# 将查询结果转换为Pandas DataFrame\n",
        "k = 0\n",
        "for line in list(cursor):\n",
        "  k = k +1\n",
        "  # 创建DataFrame\n",
        "  df = pd.DataFrame(line[\"data\"])\n",
        "  # 打印DataFrame\n",
        "\n",
        "  if k == 1:\n",
        "    display(df)\n",
        "    break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kGI5BxbXnwgq",
        "outputId": "59315e20-4e37-489c-8502-fe13af5b8524"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'_id': '353abb1cd0b75078580d09abb52498ad', 'type': 'Title', 'text': 'Tencent fii Tencent Holdings Limited'}\n",
            "3476\n"
          ]
        }
      ],
      "source": [
        "db = client['db_finance_report']\n",
        "collection = db['collection_tencent_notes']\n",
        "\n",
        "\n",
        "# Define the aggregation pipeline\n",
        "pipeline = [\n",
        "    {\n",
        "        '$match': {\n",
        "            'parent_id': None  # Assuming root documents have a parent_id of None\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        '$graphLookup': {\n",
        "            'from': collection.name,  # Use the actual collection name\n",
        "            'startWith': '$_id',  # Start with the current document's _id\n",
        "            'connectFromField': 'parent_id',  # Field in the output documents\n",
        "            'connectToField': '_id',  # Field in the searched documents\n",
        "            'as': 'tree',  # Name of the array for recursive results\n",
        "            'depthField': 'depth'  # Optional field for the depth of each node\n",
        "        }\n",
        "    },\n",
        "    {\n",
        "        '$project': {\n",
        "            '_id': 1,  # Include the _id field\n",
        "            'type': 1,  # Include the level field\n",
        "            'text': 1,  # Include the parent_id\n",
        "            # 'tree': 1,  # Include the tree array\n",
        "            # Exclude other fields by not including them in the projection\n",
        "        }\n",
        "    }\n",
        "]\n",
        "\n",
        "# Execute the aggregation pipeline\n",
        "result = collection.aggregate(pipeline)\n",
        "count = 0\n",
        "for document in result:\n",
        "    count=count+1\n",
        "    if count == 1:\n",
        "      print(document)\n",
        "    # break\n",
        "print(count)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6csgZ_1PHq"
      },
      "source": [
        "# Save files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwwbC8y0T4V",
        "outputId": "f7752008-71f9-4e79-c260-6f23a295a654"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.json-tables.html\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "All files copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print('All files copied to Google Drive.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHRAZjHc7T7v",
        "outputId": "8a7b50ba-7007-4bc1-8d39-5917a7510b4a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "All files copied to %s. /content/files\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print(f'All files copied to %s.', target_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "mount_file_id": "1FHYXQv4x6WzvoFYcHkRzzw3I8e3mX7rc",
      "authorship_tag": "ABX9TyP09OhAmxeerzdKFLgUIkM5",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}