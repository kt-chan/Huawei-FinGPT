{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kt-chan/Huawei-FinGPT/blob/master/unstructured_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZbrRMSiG8xF"
      },
      "source": [
        "# Install Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wrGiBMnQzCqk"
      },
      "outputs": [],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra libreoffice libtesseract-dev libmagic-dev"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IzBi-ChGvCXV"
      },
      "outputs": [],
      "source": [
        "!pip install unstructured unstructured-inference unstructured_pytesseract langchain openai chromadb pillow_heif pytesseract"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Setup MongoDB"
      ],
      "metadata": {
        "id": "ZkrTCcgiEqxh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# set this to ip to MongoDB atlas firewall rules, ./security/network/accessList\n",
        "!curl ifconfig.me\n",
        "!python --version"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT3b_EqlE3Dl",
        "outputId": "204431cf-5e04-4c51-ab11-a70b8d09cfab"
      },
      "execution_count": 308,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "35.199.15.218Python 3.10.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m pip install \"pymongo[srv]\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hG1ss8YsFCHt",
        "outputId": "36dc116b-e3a1-4ca7-c2b3-849f852563df"
      },
      "execution_count": 309,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pymongo[srv]\n",
            "  Downloading pymongo-4.7.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (669 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/669.1 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m399.4/669.1 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m669.1/669.1 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo[srv]) (2.6.1)\n",
            "Installing collected packages: pymongo\n",
            "Successfully installed pymongo-4.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "\n",
        "user = 'xxxxx'  ## replace with mongodb atlas username\n",
        "password = 'xxxxx' ## replace with mongodb atlas password\n",
        "uri = \"mongodb+srv://\"+user+\":\"+password+\"@ktchan-mongo-atlast.mvx53s5.mongodb.net/?retryWrites=true&w=majority&appName=ktchan-mongo-atlast\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBTTxYjiMD42",
        "outputId": "7e6963c3-06db-42fa-f139-f9ba9020a8c3"
      },
      "execution_count": 310,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeUSDVtHEhP"
      },
      "source": [
        "# PDF Content Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9JE0kv8-zW"
      },
      "source": [
        "Parittion the pdf files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UQXeITIH60Dv",
        "outputId": "d0c29a17-17d4-4547-ee53-7f1586801c26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mkdir: cannot create directory ‘./files’: File exists\n"
          ]
        }
      ],
      "source": [
        "!mkdir ./files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOttQcEva_m",
        "outputId": "bb88172b-96f1-4055-df83-8232d36bec2a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Download completed. File saved as: /content/files/downloaded_file.pdf\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "\n",
        "# For this notebook I uploaded Nvidia's earnings into the Files directory called \"/content/\"\n",
        "pdf_url = \"https://static.www.tencent.com/uploads/2024/04/08/e95c902973fc282be3b3e285c6245281.pdf\"\n",
        "output_dir = \"./files/\"\n",
        "\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    full_path = os.path.abspath(output_dir+filename)\n",
        "    with open(full_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f'Download completed. File saved as: {full_path}')\n",
        "    return full_path\n",
        "\n",
        "\n",
        "filename = download_pdf(pdf_url, 'downloaded_file.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB16l73cxbEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unstructured.staging.base import elements_to_text\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import group_broken_paragraphs\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "# Partition PDF\n",
        "# Define parameters for Unstructured's library\n",
        "strategy = \"hi_res\" # Strategy for analyzing PDFs and extracting table structure\n",
        "model_name = \"yolox\" # Best model for table extraction. Other options are detectron2_onnx and chipper depending on file layout\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "  filename=filename,\n",
        "  strategy=strategy,\n",
        "  infer_table_structure=True,\n",
        "  model_name=model_name\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Extract tables\n",
        "para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")\n",
        "element_text = elements_to_text(elements)\n",
        "element_text = group_broken_paragraphs(element_text, paragraph_split=para_split_re)\n",
        "element_text = clean_non_ascii_chars(element_text)\n",
        "\n",
        "# Store results in files\n",
        "elements_to_json(elements, filename=f\"{filename}.json\") # Takes a while for file to show up on the Google Colab\n",
        "elements_to_text(elements, filename=f\"{filename}.text\") # Takes a while for file to show up on the Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5_Wsrc-YGp"
      },
      "source": [
        "# Information Retrieval for Table Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-Klk_s9BeC"
      },
      "source": [
        "Extract tables from paritions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mswW8i_FcwHZ"
      },
      "outputs": [],
      "source": [
        "## In order to extract only the table elements I’ve written a helper function to do so:\n",
        "import json\n",
        "from html import escape\n",
        "\n",
        "def process_json_file(input_filename):\n",
        "  # Read the JSON file\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "\n",
        "    # Iterate over the JSON data and extract required table elements\n",
        "    extracted_elements = []\n",
        "    for entry in data:\n",
        "      if entry[\"type\"] == \"Table\":\n",
        "        entry[\"metadata\"][\"element_id\"] = entry[\"element_id\"]\n",
        "        extracted_elements.append(entry[\"metadata\"])\n",
        "\n",
        "  # Write the extracted elements to the output file\n",
        "  with open(output_dir+\"/downloaded_file.pdf.json-tables.html\", 'w') as output_file:\n",
        "    for element in extracted_elements:\n",
        "      output_file.write(\"<span id=\\'\" + element[\"element_id\"]  + \"\\' metadata=\\'\")\n",
        "      text_as_html = element.pop('text_as_html')\n",
        "      # Convert the dictionary to a JSON string\n",
        "      json_string = json.dumps(element)\n",
        "      escaped_json_string = escape(json_string)\n",
        "      output_file.write(escaped_json_string)\n",
        "      output_file.write(\"\\'>\")\n",
        "      output_file.write(text_as_html) # Adding two newlines for separation\n",
        "      output_file.write(\"</span>\" + \"\\n\\n\")\n",
        "\n",
        "process_json_file(f\"{filename}.json\") # Takes a while for the .txt file to show up in Colab\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 251,
      "metadata": {
        "id": "hUS9qEondDku"
      },
      "outputs": [],
      "source": [
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "\n",
        "\n",
        "# Define a fuctnion to clean tables headings\n",
        "def clean_table(ds_table):\n",
        "    df_table =  pd.DataFrame(ds_table)\n",
        "    df_numeric = df_table.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "    df_combined = pd.concat([df_table.iloc[:, 0], df_numeric], axis=1)\n",
        "    df_cleaned = df_combined.dropna(how='any')\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def clean_text(text):\n",
        "    # Check if the text is a string\n",
        "    if isinstance(text, str):\n",
        "        # Replace unwanted characters if it's a string\n",
        "        text = text.replace('#', '').replace('*', '')\n",
        "        # Check if the text is a number with commas\n",
        "        if re.match(r'^-?\\d{1,3}(,\\d{3})*\\.\\d+$', text):\n",
        "            # Remove commas and convert to float\n",
        "            return float(text.replace(',', ''))\n",
        "        # Check if the text is an integer with commas\n",
        "        elif re.match(r'^-?\\d{1,3}(,\\d{3})*$', text):\n",
        "            # Remove commas and convert to integer\n",
        "            return int(text.replace(',', ''))\n",
        "    # Return the text as is if it's not a string\n",
        "    return text\n",
        "\n",
        "\n",
        "html_content = None\n",
        "# Your HTML content as a string\n",
        "file_path = output_dir + '/downloaded_file.pdf.json-tables.html'\n",
        "# Write the HTML content to the file\n",
        "with open(file_path, 'r', encoding='utf-8') as file:\n",
        "    html_content = file.read()\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "\n",
        "# Parse the HTML content using BeautifulSoup\n",
        "soup = BeautifulSoup(html_content, 'html.parser')\n",
        "\n",
        "# Initialize an empty dictionary to hold the tables with span_id as the key\n",
        "# and the table data as the value\n",
        "tables_dict = {}\n",
        "\n",
        "\n",
        "# Find all span elements with an 'id' attribute\n",
        "for span in soup.find_all('span', id=True):\n",
        "    span_id = span['id']\n",
        "\n",
        "    # Initialize a dictionary to hold the metadata and table data for the current span\n",
        "    span_data = {}\n",
        "\n",
        "    # Extract the 'metadata' attribute if it exists\n",
        "    metadata = span.get('metadata', None)\n",
        "    if metadata:\n",
        "        span_data['metadata'] = metadata\n",
        "\n",
        "    # Find the table within the span element\n",
        "    table = span.find('table')\n",
        "    if table:\n",
        "        # Use pandas to read the table\n",
        "        table_df = pd.read_html(str(table)) # Get the first DataFrame\n",
        "        span_data['table'] = table_df\n",
        "\n",
        "    # Add the span data to the main dictionary using the span_id as the key\n",
        "    tables_dict[span_id] = span_data\n",
        "\n",
        "\n",
        "# Get the first key-value pair based on insertion order\n",
        "datasets = []\n",
        "for span_id, span_data in tables_dict.items():\n",
        "    # Access the metadata and the table DataFrame\n",
        "    metadata = span_data.get('metadata')\n",
        "    tables_df_tmp = span_data.get('table')\n",
        "    tables_df= []\n",
        "    for ds_table in tables_df_tmp:\n",
        "      ds_table = ds_table.applymap(clean_text)\n",
        "      tables_df.append(ds_table)\n",
        "\n",
        "    datasets.append({\n",
        "            \"id\": span_id,  # Corrected the syntax for dictionary keys (no quotes)\n",
        "            \"meta\": metadata,  # Corrected the variable name ('metadata' instead of 'metdata')\n",
        "            \"data\": tables_df,  # Assuming you want to store the first DataFrame in the list\n",
        "            \"data_raw\": tables_df_tmp  # raw data format, without formatting\n",
        "        })\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This is for testing the maths group function on clean tables."
      ],
      "metadata": {
        "id": "-VU9tmNY8Jk-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## This is for testing the maths group function on clean tables.\n",
        "idx = 0\n",
        "\n",
        "# for dataset in datasets[idx][\"data\"]:\n",
        "#   dataset = clean_table(dataset)\n",
        "#   dataset = dataset.iloc[:,1:]\n",
        "#   dataset = dataset.sum().to_frame(name=\"sum\")\n",
        "#   display(dataset)\n",
        "#   break\n",
        "\n",
        "for dataset in datasets[idx][\"data\"]:\n",
        "  display(dataset)\n",
        "  break\n",
        "\n",
        "\n",
        "data_dict = None\n",
        "for dataset in datasets[idx][\"data\"]:\n",
        "    data_dict = dataset.to_dict(\"records\")\n",
        "    display(data_dict)\n",
        "    break;\n",
        "\n",
        "df = pd.DataFrame(data_dict)\n",
        "display(clean_table(df))"
      ],
      "metadata": {
        "id": "kGI5BxbXnwgq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6csgZ_1PHq"
      },
      "source": [
        "# Save files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwwbC8y0T4V",
        "outputId": "f7752008-71f9-4e79-c260-6f23a295a654"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.json-tables.html\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "All files copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print('All files copied to Google Drive.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "authorship_tag": "ABX9TyP+k/KwGPs8EopHQOM/Zanc",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}