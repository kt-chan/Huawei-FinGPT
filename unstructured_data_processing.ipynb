{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kt-chan/Huawei-FinGPT/blob/master/unstructured_data_processing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZbrRMSiG8xF"
      },
      "source": [
        "# Install Libraries\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 97,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wrGiBMnQzCqk",
        "outputId": "982b1bd9-df94-4125-df37-d0a9630916e7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rHit:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n"
          ]
        }
      ],
      "source": [
        "!apt-get update\n",
        "!apt-get install -y poppler-utils tesseract-ocr tesseract-ocr-chi-sim tesseract-ocr-chi-tra libreoffice libtesseract-dev libmagic-dev  > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Recovery Files from drive"
      ],
      "metadata": {
        "id": "W7c1A6xAcd-0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/llms'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/llms'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print(f'All files copied to %s.', target_dir)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_brXLcowccjg",
        "outputId": "a75ad73c-d94d-43ea-841e-c1aaf9ed0f06"
      },
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.txt\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "File copied: td-374.txt\n",
            "File copied: zhipuchat.py\n",
            "File copied: __init__.py\n",
            "File copied: requirements.txt\n",
            "File copied: kimichat.py\n",
            "All files copied to %s. /content/llms\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 96,
      "metadata": {
        "collapsed": true,
        "id": "IzBi-ChGvCXV"
      },
      "outputs": [],
      "source": [
        "!pip install -r ./llms/requirements.txt  > /dev/null 2>&1\n",
        "!pip install unstructured[pdf] > /dev/null 2>&1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkrTCcgiEqxh"
      },
      "source": [
        "# Setup MongoDB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g2zVG69g45wm"
      },
      "source": [
        "Connection Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CT3b_EqlE3Dl",
        "outputId": "984d8b95-51d3-4d59-e5bb-ca95ee545671"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.44.45.112\n",
            "Requirement already satisfied: pymongo[srv] in /usr/local/lib/python3.10/dist-packages (4.8.0)\n",
            "\u001b[33mWARNING: pymongo 4.8.0 does not provide the extra 'srv'\u001b[0m\u001b[33m\n",
            "\u001b[0mRequirement already satisfied: dnspython<3.0.0,>=1.16.0 in /usr/local/lib/python3.10/dist-packages (from pymongo[srv]) (2.6.1)\n"
          ]
        }
      ],
      "source": [
        "# set this to ip to MongoDB atlas firewall rules, https://cloud.mongodb.com/v2/667549db2c13183084c47650#/security/network/accessList\n",
        "!curl ifconfig.me\n",
        "print()\n",
        "!python -m pip install \"pymongo[srv]\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBTTxYjiMD42",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f9b24971-eb3c-435f-92cb-7a7e944e5c18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pinged your deployment. You successfully connected to MongoDB!\n"
          ]
        }
      ],
      "source": [
        "from pymongo.mongo_client import MongoClient\n",
        "from pymongo.server_api import ServerApi\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "user = userdata.get('mongodb_username')  ## replace with mongodb atlas username\n",
        "password = userdata.get('mongodb_password')## replace with mongodb atlas password\n",
        "uri = \"mongodb+srv://\"+user+\":\"+password+\"@ktchan-mongo-atlast.mvx53s5.mongodb.net/?retryWrites=true&w=majority&appName=ktchan-mongo-atlast\"\n",
        "\n",
        "# Create a new client and connect to the server\n",
        "client = MongoClient(uri, server_api=ServerApi('1'))\n",
        "\n",
        "# Send a ping to confirm a successful connection\n",
        "try:\n",
        "    client.admin.command('ping')\n",
        "    print(\"Pinged your deployment. You successfully connected to MongoDB!\")\n",
        "except Exception as e:\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MGeUSDVtHEhP"
      },
      "source": [
        "# Content Extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TU9JE0kv8-zW"
      },
      "source": [
        "Parittion the pdf files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MEOttQcEva_m",
        "outputId": "ff37a8f6-2d23-4a92-bb9b-a873f5f5e806"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Download completed. File saved as: /content/files/towngas_downloaded_file.pdf\n"
          ]
        }
      ],
      "source": [
        "import os, requests\n",
        "from unstructured.partition.pdf import partition_pdf\n",
        "from unstructured.staging.base import elements_to_json\n",
        "\n",
        "# For this notebook I uploaded Nvidia's earnings into the Files directory called \"/content/\"\n",
        "# pdf_url = \"https://static.www.tencent.com/uploads/2024/04/08/e95c902973fc282be3b3e285c6245281.pdf\"\n",
        "pdf_url = \"https://www.towngas.com/getmedia/995f95f0-0532-4770-af3b-9245e03a2e22/Annual-Report-2023-C.pdf.aspx?ext=.pdf\"\n",
        "output_dir = \"./files/\"\n",
        "\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    full_path = os.path.abspath(output_dir+filename)\n",
        "    with open(full_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f'Download completed. File saved as: {full_path}')\n",
        "    return full_path\n",
        "\n",
        "\n",
        "filename = download_pdf(pdf_url, 'towngas_downloaded_file.pdf')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sB16l73cxbEr"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "from unstructured.staging.base import elements_to_text\n",
        "from unstructured.cleaners.core import clean_non_ascii_chars\n",
        "from unstructured.cleaners.core import group_broken_paragraphs\n",
        "from unstructured.chunking.title import chunk_by_title\n",
        "\n",
        "# Partition PDF\n",
        "# Define parameters for Unstructured's library\n",
        "strategy = \"hi_res\" # Strategy for analyzing PDFs and extracting table structure\n",
        "model_name = \"yolox\" # Best model for table extraction. Other options are detectron2_onnx and chipper depending on file layout\n",
        "\n",
        "# Extracts the elements from the PDF\n",
        "elements = partition_pdf(\n",
        "  filename=filename,\n",
        "  strategy=strategy,\n",
        "  infer_table_structure=True,\n",
        "  model_name=model_name\n",
        ")\n",
        "\n",
        "\n",
        "\n",
        "# Extract tables\n",
        "para_split_re = re.compile(r\"(\\s*\\n\\s*){3}\")\n",
        "element_text = elements_to_text(elements)\n",
        "element_text = group_broken_paragraphs(element_text, paragraph_split=para_split_re)\n",
        "element_text = clean_non_ascii_chars(element_text)\n",
        "\n",
        "# Store results in files\n",
        "elements_to_json(elements, filename=f\"{filename}.json\") # Takes a while for file to show up on the Google Colab\n",
        "elements_to_text(elements, filename=f\"{filename}.text\") # Takes a while for file to show up on the Google Colab"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQ5_Wsrc-YGp"
      },
      "source": [
        "# Table Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mswW8i_FcwHZ",
        "outputId": "c68010d2-d806-4221-c9b8-73510b4e4fa2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Written 158 tables documents into mongodb collection_tencent.\n",
            "Written 3476 element documents into mongodb collection_tencent_notes.\n",
            "Element Tree size is 3477.\n"
          ]
        }
      ],
      "source": [
        "## In order to extract only the table elements I’ve written a helper function to do so:\n",
        "import re\n",
        "import json\n",
        "from html import escape\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "from IPython.display import display\n",
        "from treelib import Node, Tree\n",
        "from io import StringIO\n",
        "\n",
        "\n",
        "def extract_json_elements(input_filename, element_tree):\n",
        "  # Read the JSON file\n",
        "  extracted_elements = {}\n",
        "\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required elements\n",
        "    for entry in data:\n",
        "      text = entry[\"text\"]\n",
        "\n",
        "      if \"Table\" == entry[\"type\"]:\n",
        "          text = entry[\"metadata\"][\"text_as_html\"]\n",
        "\n",
        "      if 'parent_id' in entry['metadata']:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "          \"_id\": entry[\"element_id\"],\n",
        "          \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "          \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "          \"parent_id\":entry[\"metadata\"][\"parent_id\"],\n",
        "          \"type\":entry[\"type\"],\n",
        "          \"text\":text\n",
        "          }\n",
        "      else:\n",
        "        extracted_elements[entry[\"element_id\"]] = {\n",
        "            \"_id\": entry[\"element_id\"],\n",
        "            \"filename\":entry[\"metadata\"][\"filename\"],\n",
        "            \"page_number\":entry[\"metadata\"][\"page_number\"],\n",
        "            \"type\":entry[\"type\"],\n",
        "            \"text\":text\n",
        "            }\n",
        "\n",
        "  build_dependency(extracted_elements, element_tree)\n",
        "  return extracted_elements\n",
        "\n",
        "def build_dependency(extracted_elements, element_tree):\n",
        "    # construct element dependency tree\n",
        "  for k, element in extracted_elements.items():\n",
        "      if \"parent_id\" in element:\n",
        "        # print(element)\n",
        "        parent_target = element[\"parent_id\"]\n",
        "        node = element_tree.get_node(parent_target)\n",
        "        if node:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "        else:\n",
        "          element.pop(\"parent_id\", None)\n",
        "          # element_parent = extracted_elements[parent_target].pop(\"parent_id\", None)\n",
        "          element_tree.create_node(parent_target, parent_target, parent=\"root\", data = extracted_elements[parent_target])\n",
        "          element_tree.create_node(k, k, parent=parent_target, data = element)\n",
        "      else:\n",
        "        element_tree.create_node(k, k, parent=\"root\", data = element)\n",
        "\n",
        "\n",
        "def extract_json_table(input_filename):\n",
        "  # Read the JSON file\n",
        "  elements_table = []\n",
        "  with open(input_filename, 'r') as file:\n",
        "    data = json.load(file)\n",
        "    # Iterate over the JSON data and extract required table elements\n",
        "    for entry in data:\n",
        "      if entry[\"type\"] == \"Table\":\n",
        "        entry[\"metadata\"][\"element_id\"] = entry[\"element_id\"]\n",
        "        elements_table.append({\"element_id\":entry[\"element_id\"], \"metadata\" : entry[\"metadata\"]})\n",
        "\n",
        "  return parse_html_table(elements_table)\n",
        "\n",
        "\n",
        "#Define a function to clean table columns\n",
        "def extract_table_from_html(table_html_string):\n",
        "  # Rename the columns to be just the first element of each tuple\n",
        "  # Function to convert elements to string\n",
        "  def to_str(element):\n",
        "      if isinstance(element, tuple):\n",
        "          return '_'.join(map(str, element))  # Join tuple elements with an underscore\n",
        "      else:\n",
        "          return str(element)  # Convert integer to string\n",
        "\n",
        "  # Find the table within the span element\n",
        "  table = table_html_string.find('table')\n",
        "  df_flattened = None\n",
        "  if table:\n",
        "      table_dfs = pd.read_html(StringIO(table_html_string)) # Get the first DataFrame\n",
        "      table_df = table_dfs[0] # assume only one table per json tab\n",
        "      df_flattened = table_df.map(str)  # Convert all values to string\n",
        "      df_flattened.columns = [to_str(col) for col in df_flattened.columns] # Column headers parsed as set by pdf extractor, force to string for compatibility\n",
        "\n",
        "  return df_flattened\n",
        "\n",
        "\n",
        "# Define a fuctnion to formatn tables cells\n",
        "def format_table(df_table):\n",
        "    # df_table =  pd.DataFrame(ds_table)\n",
        "    df_numeric = df_table.iloc[:, 1:].apply(pd.to_numeric, errors='coerce')\n",
        "    df_combined = pd.concat([df_table.iloc[:, 0], df_numeric], axis=1)\n",
        "    df_cleaned = df_combined.dropna(how='any')\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def clean_text(text):\n",
        "\n",
        "    # Function to convert matched text to negative number\n",
        "    def make_negative(match):\n",
        "        number = match.group(1).replace(',', '')\n",
        "        return f\"{-int(number)}\"\n",
        "\n",
        "    # Check if the text is a string\n",
        "    if isinstance(text, str):\n",
        "        # Replace unwanted characters if it's a string\n",
        "        text = text.replace('#', '').replace('*', '')\n",
        "\n",
        "\n",
        "        # Regular expression to match numbers in parentheses with commas\n",
        "        pattern = r'\\((\\d{1,3}(,\\d{3})*)\\)'\n",
        "        # Replace all occurrences of the pattern with its negative equivalent\n",
        "        text = re.sub(pattern, make_negative, text)\n",
        "\n",
        "        # Check if the text is a number with commas\n",
        "        if re.match(r'^-?\\d{1,3}(,\\d{3})*\\.\\d+$', text):\n",
        "            # Remove commas and convert to float\n",
        "            return float(text.replace(',', ''))\n",
        "        # Check if the text is an integer with commas\n",
        "        elif re.match(r'^-?\\d{1,3}(,\\d{3})*$', text):\n",
        "            # Remove commas and convert to integer\n",
        "            return int(text.replace(',', ''))\n",
        "    # Return the text as is if it's not a string\n",
        "    return text\n",
        "\n",
        "\n",
        "# Define a function to clean the data\n",
        "def parse_html_table(data_tables):\n",
        "\n",
        "  # Initialize an empty dictionary to hold the tables with span_id as the key\n",
        "  # and the table data as the value\n",
        "  tables_dict = {}\n",
        "\n",
        "\n",
        "  # Find all span elements with an 'id' attribute\n",
        "  for data_table in data_tables:\n",
        "      span_id = data_table['element_id']\n",
        "\n",
        "      # Initialize a dictionary to hold the metadata and table data for the current span\n",
        "      span_data = {}\n",
        "\n",
        "      # Extract the 'metadata' attribute if it exists\n",
        "      metadata = data_table.get('metadata', None)\n",
        "\n",
        "      if metadata:\n",
        "          span_data['metadata'] = metadata\n",
        "\n",
        "      # Find the table within the span element\n",
        "      table = data_table.get('text_as_html', None)\n",
        "      if table:\n",
        "          # Use pandas to read the table\n",
        "          table_df = pd.read_html(StringIO(table)) # Get the first DataFrame\n",
        "          span_data['table'] = table_df\n",
        "\n",
        "      # Add the span data to the main dictionary using the span_id as the key\n",
        "      tables_dict[span_id] = span_data\n",
        "\n",
        "\n",
        "  datasets = []\n",
        "\n",
        "  # Get the first key-value pair based on insertion order\n",
        "  for span_id, span_data in tables_dict.items():\n",
        "      # Access the metadata and the table DataFrame\n",
        "      metadata = span_data.get('metadata')\n",
        "      table_html = metadata.get('text_as_html')\n",
        "      df_table = extract_table_from_html(table_html)\n",
        "      df_table_clean = df_table.map(clean_text)\n",
        "      datasets.append({\n",
        "          \"_id\": span_id,  # Corrected the syntax for dictionary keys (no quotes)\n",
        "          \"meta\": metadata,  # Corrected the variable name ('metadata' instead of 'metdata')\n",
        "          \"data\": df_table_clean.to_dict(\"records\"),  # Assuming you want to store the first DataFrame in the list\n",
        "          \"data_raw\": df_table.to_dict(\"records\")  # raw data format, without formatting\n",
        "          })\n",
        "\n",
        "  return datasets\n",
        "\n",
        "\n",
        "file_path=\"/content/files/downloaded_file.pdf.json\"\n",
        "element_tree = Tree()\n",
        "element_tree.create_node(\"root\", \"root\")\n",
        "elements = extract_json_elements(file_path, element_tree)\n",
        "elements_tables = extract_json_table(file_path)\n",
        "\n",
        "\n",
        "db = client['db_finance_report']\n",
        "collection = db['collection_tencent']\n",
        "collection.drop()\n",
        "insert_collection = elements_tables\n",
        "collection.insert_many(insert_collection);\n",
        "print(f'Written {collection.count_documents({})} tables documents into mongodb collection_tencent.')\n",
        "\n",
        "\n",
        "collection = db['collection_tencent_notes']\n",
        "collection.drop()\n",
        "collection.insert_many(list(elements.values()));\n",
        "print(f'Written {collection.count_documents({})} element documents into mongodb collection_tencent_notes.')\n",
        "\n",
        "\n",
        "print(f'Element Tree size is {element_tree.size()}.')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SA-Klk_s9BeC"
      },
      "source": [
        "Extract tables from paritions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z5-C5KuaBV8W"
      },
      "outputs": [],
      "source": [
        "# print(element_tree)\n",
        "\n",
        "# Depth-First Search (DFS) traversal\n",
        "def dfs_traversal(node: Node, leaf_root: Node, tree: Tree):\n",
        "    if node:\n",
        "      if leaf_root.data and 'text' in leaf_root.data:\n",
        "         if not node.is_root():\n",
        "            prefix = f\"<span id='{leaf_root.identifier}'>\" + node.data[\"text\"] + \"</span>\"\n",
        "            leaf_root.data[\"text\"]  =  prefix + leaf_root.data[\"text\"]\n",
        "            # print(\"</br>\" + leaf_root.data[\"text\"] + \"</br>\")\n",
        "      dfs_traversal(tree.parent(node.identifier), leaf_root, tree)\n",
        "\n",
        "# Start the traversal from the root node\n",
        "# target_node = element_tree.get_node(\"0ce1e2d0bb4281b12e30ad32b5653f36\")\n",
        "for table in elements_tables:\n",
        "  leaf = element_tree.get_node(table[\"_id\"])\n",
        "  if leaf:\n",
        "    parent = element_tree.parent(leaf.identifier)\n",
        "    if(leaf and parent):\n",
        "      dfs_traversal(parent, leaf, element_tree)\n",
        "  else:\n",
        "    print(table)\n",
        "\n",
        "\n",
        "# Function to print all data in the tree\n",
        "def print_all_data(tree, filename):\n",
        "  with open(filename, 'w') as file:  # Open the file in write mode\n",
        "    for node in tree.all_nodes():\n",
        "        # Check if 'text' key exists in the data dictionary\n",
        "        if node.data and 'text' in node.data:\n",
        "            # Print the value associated with 'text' key\n",
        "             file.write(f\"{node.data['text']}\\n\")  #\n",
        "\n",
        "# Call the function to print all data\n",
        "print_all_data(element_tree, \"./files/downloaded_file.txt\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM Setup"
      ],
      "metadata": {
        "id": "Fd8vIMWaetjB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile /content/llms/zhipuchat.py\n",
        "## This is auto generated from colab scripts.\n",
        "import os\n",
        "from pathlib import Path\n",
        "from zhipuai import ZhipuAI\n",
        "from dotenv import load_dotenv\n",
        "import httpx\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "class CustomClient(httpx.Client):\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize httpx.Client with verify=False to disable SSL verification\n",
        "        super().__init__(verify=False, *args, **kwargs)\n",
        "\n",
        "\n",
        "class ZhipuChat():\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        # Initialize httpx.Client with verify=False to disable SSL verification\n",
        "        # Load .env file\n",
        "        load_dotenv()\n",
        "        # # Initialize the OpenAI client with the custom HTTP client\n",
        "        # self._client = OpenAI(\n",
        "        #     api_key=userdata.get('ZHIPU_API_KEY'),\n",
        "        #     base_url=\"https://open.bigmodel.cn/api/paas/v4/\",\n",
        "        #     http_client= CustomClient()  # Use the custom HTTP client\n",
        "        # )\n",
        "        self._client = ZhipuAI(api_key=userdata.get('ZHIPU_API_KEY'), http_client=CustomClient())\n",
        "        self._ragid = \"1810981053545041920\"\n",
        "        # self._ragid = self._client.knowledge.create(\n",
        "        #     embedding_id=3,\n",
        "        #     name=\"default\",\n",
        "        #     description=\"default knowledge base\"\n",
        "        # )\n",
        "\n",
        "    def getclient(self):\n",
        "        return self._client\n",
        "\n",
        "    def getvectordbid(self):\n",
        "        return str(self._ragid)\n",
        "\n",
        "    def getcompletion(self, messages):\n",
        "        completion = self.getclient().chat.completions.create(\n",
        "            model=\"glm-4-flash\",\n",
        "            messages=messages,\n",
        "            max_tokens=4095,\n",
        "            top_p=0.7,\n",
        "            temperature=0.9\n",
        "        )\n",
        "        return completion\n",
        "\n",
        "    def getcompletionrag(self, message, tools):\n",
        "\n",
        "      \t# completion = self.getclient().chat.completions.create(\n",
        "        #     model=\"glm-4-flash\",\n",
        "        #     messages=[\n",
        "        #         {\"role\": \"user\", \"content\": \"你好！你叫什么名字\"},\n",
        "        #     ],\n",
        "        #     tools=[\n",
        "        #             {\n",
        "        #                 \"type\": \"retrieval\",\n",
        "        #                 \"retrieval\": {\n",
        "        #                     \"knowledge_id\": \"your knowledge id\",\n",
        "        #                     \"prompt_template\": \"从文档\\n\\\"\\\"\\\"\\n{{knowledge}}\\n\\\"\\\"\\\"\\n中找问题\\n\\\"\\\"\\\"\\n{{question}}\\n\\\"\\\"\\\"\\n的答案，找到答案就仅使用文档语句回答问题，找不到答案就用自身知识回答并且告诉用户该信息不是来自文档。\\n不要复述问题，直接开始回答。\"\n",
        "        #                 }\n",
        "        #             }\n",
        "        #             ],\n",
        "        #     stream=True,\n",
        "        # )\n",
        "        print(message)\n",
        "        print(tools)\n",
        "        completion = self.getclient().chat.completions.create(\n",
        "            model=\"glm-4-flash\",\n",
        "            messages=message,\n",
        "            tools=tools,\n",
        "            stream=False\n",
        "        )\n",
        "        return completion\n",
        "\n",
        "    def contentupload(self, filepath):\n",
        "        vdbid=self.getvectordbid()\n",
        "        print(\"uploading to knowledge base id: \" + vdbid)\n",
        "        resp = self.getclient().knowledge.document.create(\n",
        "            file=open(filepath, \"rb\"),\n",
        "            purpose=\"retrieval\",\n",
        "            knowledge_id=vdbid,\n",
        "            # sentence_size=202,\n",
        "            custom_separator=[\"\\n\"]\n",
        "        )\n",
        "        return resp\n",
        "\n",
        "    def chatping(self):\n",
        "        # Check if completion was successful and print the message\n",
        "        completion = self.getcompletion([\n",
        "            {\"role\": \"system\", \"content\": \"你是人工智能助手...\"},\n",
        "            {\"role\": \"user\", \"content\": \"你好，我叫李雷，1+1等于多少？\"}\n",
        "        ])\n",
        "        if completion and completion.choices:\n",
        "            print(completion.choices[0].message.content)\n",
        "\n",
        "    def chatrag(self, question):\n",
        "        print(question + \"\\n\\n\")\n",
        "\n",
        "        messages = [\n",
        "            {\n",
        "                \"role\": \"system\",\n",
        "                \"content\": \"\"\"\n",
        "                你是专业的文档大纲分析专员。你的任务是从长文档中提取关键信息，生成摘要和大纲。你的能力有:\n",
        "                - 分析文档结构:自动识别文档章节，提炼重点内容。\n",
        "                - 生成摘要:概括文档核心思想，使之简洁明了。\n",
        "                - 构建大纲:梳理文档逻辑，展现章节关系。\n",
        "                - 输出结果:以JSON格式呈现，包含摘要、大纲和主题等关键信息。\n",
        "                \"\"\",\n",
        "            },\n",
        "            {\"role\": \"user\", \"content\": question},\n",
        "        ]\n",
        "\n",
        "        tools = [\n",
        "            {\n",
        "                \"type\": \"retrieval\",\n",
        "                \"retrieval\": {\n",
        "                    \"knowledge_id\": self.getvectordbid(),\n",
        "                    \"prompt_template\": \"从文档\\n\\\"\\\"\\\"\\n{{knowledge}}\\n\\\"\\\"\\\"\\n中找问题\\n\\\"\\\"\\\"\\n{{question}}\\n\\\"\\\"\\\"\\n的答案，找到答案就仅使用文档语句回答问题，找不到答案就用自身知识回答并且告诉用户该信息不是来自文档。\\n不要复述问题，直接开始回答。\"\n",
        "                }\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        completion = self.getcompletionrag(messages, tools)\n",
        "        return completion\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9NgzEeIrI4P1",
        "outputId": "8b9a4f5b-c6b4-48e1-b96f-1ad0e77b1ea3"
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting /content/llms/zhipuchat.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext autoreload\n",
        "%reload_ext autoreload"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "czoCssif42Y2",
        "outputId": "e90614e9-0d6b-4cc8-f445-c8be4807c8c5"
      },
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nao00jRUNfdJ"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kGI5BxbXnwgq"
      },
      "outputs": [],
      "source": [
        "import os, requests, importlib\n",
        "import llms.zhipuchat\n",
        "\n",
        "importlib.reload(llms.zhipuchat)\n",
        "\n",
        "agentchat = ZhipuChat()\n",
        "\n",
        "pdf_url = \"https://raw.githubusercontent.com/kt-chan/UnstructredExtractor/main/cap374.txt\"\n",
        "output_file = \"/content/files/td-374.txt\"\n",
        "\n",
        "def download_pdf(url, filename):\n",
        "    response = requests.get(url, stream=True)\n",
        "    full_path = os.path.abspath(filename)\n",
        "    with open(full_path, 'wb') as f:\n",
        "        f.write(response.content)\n",
        "    print(f'Download completed. File saved as: {full_path}')\n",
        "    return full_path\n",
        "\n",
        "\n",
        "def uploadInfo(filepath):\n",
        "    agentchat.contentupload(filepath)\n",
        "\n",
        "uploadfile=download_pdf(pdf_url, output_file)\n",
        "uploadInfo(uploadfile)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os, requests, importlib\n",
        "import llms.zhipuchat\n",
        "\n",
        "importlib.reload(llms.zhipuchat)\n",
        "\n",
        "agentchat = ZhipuChat()\n",
        "response = agentchat.chatrag(\"提取文中主题，要详细覆盖文中所有章节。以Json Array list输出。\")\n",
        "\n",
        "# for chunk in response:\n",
        "#   print(chunk.choices[0].delta)\n",
        "\n",
        "# Check if the response contains 'choices' and access the first choice\n",
        "if hasattr(response, 'choices') and response.choices:\n",
        "    for choice in response.choices:\n",
        "        # Check if the choice has a 'message' attribute\n",
        "        if hasattr(choice, 'message') and choice.message:\n",
        "            # Access the content of the message\n",
        "            content = choice.message.content\n",
        "            print(content.replace('```json', '').replace('```', ''))\n",
        "else:\n",
        "    print(\"No choices found in the response.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4XMOKZOKRQF",
        "outputId": "2d32a17b-5a55-4c9e-fea1-60430dc021f1"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "提取文中主题，要详细覆盖文中所有章节。以Json Array list输出。\n",
            "\n",
            "\n",
            "[{'role': 'system', 'content': '\\n                你是专业的文档大纲分析专员。你的任务是从长文档中提取关键信息，生成摘要和大纲。你的能力有:\\n                - 分析文档结构:自动识别文档章节，提炼重点内容。\\n                - 生成摘要:概括文档核心思想，使之简洁明了。\\n                - 构建大纲:梳理文档逻辑，展现章节关系。\\n                - 输出结果:以JSON格式呈现，包含摘要、大纲和主题等关键信息。\\n                '}, {'role': 'user', 'content': '提取文中主题，要详细覆盖文中所有章节。以Json Array list输出。'}]\n",
            "[{'type': 'retrieval', 'retrieval': {'knowledge_id': '1810981053545041920', 'prompt_template': '从文档\\n\"\"\"\\n{{knowledge}}\\n\"\"\"\\n中找问题\\n\"\"\"\\n{{question}}\\n\"\"\"\\n的答案，找到答案就仅使用文档语句回答问题，找不到答案就用自身知识回答并且告诉用户该信息不是来自文档。\\n不要复述问题，直接开始回答。'}}]\n",
            "\n",
            "[\n",
            "  {\n",
            "    \"title\": \"Application for Driving Instructors' Licences for Restricted Driving Instructors\",\n",
            "    \"content\": \"A person who wishes to obtain a driving instructor's licence of a restricted driving instructor in respect of any group of motor vehicles shall deliver to the Commissioner an application in a form specified by the Commissioner and signed by the person; and the person's identity document.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Application for Private Driving Instructors' Licences\",\n",
            "    \"content\": \"The Commissioner may issue private driving instructors' licences for a group of motor vehicles if it is desirable, and may invite applications from specified classes of persons.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Driving Instructors' Tests\",\n",
            "    \"content\": \"A driving instructor's test may be divided into parts, and no part shall be taken until the applicant has passed any preceding part. The Commissioner may appoint an authorized examiner to conduct the test, and the applicant must pass the test to demonstrate ability and fitness to give driving instruction.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Driving Instructors' Induction Courses\",\n",
            "    \"content\": \"A driving instructor's induction course may only be provided by an authorized trainer, and the Commissioner must specify a period within which the course must be completed.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Driving Instructors' Refresher Courses\",\n",
            "    \"content\": \"A driving instructor's refresher course may only be provided by an authorized trainer.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Test of Continued Ability and Fitness to Give Driving Instruction\",\n",
            "    \"content\": \"The Commissioner may require a driving instructor to take a test of continued ability and fitness to give driving instruction, and the instructor must pass the test to demonstrate ability and fitness.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Quotas for Specified Persons\",\n",
            "    \"content\": \"The Commissioner may determine quotas for specified persons when issuing private driving instructors' licences.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Notice of Invitations for Applications\",\n",
            "    \"content\": \"The Commissioner may publish notices inviting applications for private driving instructors' licences, specifying the number of licences to be issued and a date by which applications must be received.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Driving Instructors' Licence Applications\",\n",
            "    \"content\": \"A person applying for a private driving instructor's licence must deliver an application form and identity document to the Commissioner by the specified date.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Handling of Excess Applications\",\n",
            "    \"content\": \"If the number of applications exceeds the number of licences to be issued, the Commissioner may determine the order in which applications are to be dealt with by lot.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Refusal of Licences for Multiple Applications\",\n",
            "    \"content\": \"The Commissioner may refuse to issue a private driving instructor's licence to a person who delivers more than one application in response to an invitation.\"\n",
            "  },\n",
            "  {\n",
            "    \"title\": \"Interpretation of Terms\",\n",
            "    \"content\": \"The regulations provide definitions for terms such as 'applicant', 'authorized examiner', 'authorized trainer', 'class', 'designated group', 'driving instructor', and others.\"\n",
            "  }\n",
            "]\n",
            "\n",
            "\n",
            "请注意，部分主题的信息可能需要结合自身知识进行补充，因为这些信息可能没有直接在文档中找到。\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GQ6csgZ_1PHq"
      },
      "source": [
        "# Save files to Drive"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ./files ./llms"
      ],
      "metadata": {
        "id": "K6PckLVP9eoG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 93,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abwwbC8y0T4V",
        "outputId": "555256dc-6382-4be2-a5c4-19e14ea44397"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.txt\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: td-374.txt\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf\n",
            "File copied: requirements.txt\n",
            "File copied: zhipuchat.py\n",
            "File copied: __init__.py\n",
            "File copied: kimichat.py\n",
            "All files copied to Google Drive.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "source_dir = '/content/llms'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "target_dir = '/content/drive/MyDrive/Colab Notebooks/llms'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print('All files copied to Google Drive.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mHRAZjHc7T7v",
        "outputId": "ee14ed7b-cbc8-482d-f1a6-a9952fd8f6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "File copied: downloaded_file.txt\n",
            "File copied: downloaded_file.pdf.json\n",
            "File copied: downloaded_file.pdf.text\n",
            "File copied: downloaded_file.pdf\n",
            "File copied: zhipuchat.py\n",
            "File copied: __init__.py\n",
            "File copied: requirements.txt\n",
            "File copied: kimichat.py\n",
            "All files copied to %s. /content/llms\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import shutil\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "# Mount your Google Drive to the Colab environment\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/files'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/files'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "\n",
        "# Define the source directory (local to the Colab environment)\n",
        "target_dir = '/content/llms'  # Update this to the correct path of your \"output\" directory\n",
        "\n",
        "# Define the target directory in your Google Drive\n",
        "source_dir = '/content/drive/MyDrive/Colab Notebooks/llms'  # Update this to your desired path\n",
        "\n",
        "# Make sure the target directory exists, if not create it\n",
        "if not os.path.exists(target_dir):\n",
        "    os.makedirs(target_dir)\n",
        "\n",
        "\n",
        "# Copy all files from the source directory to the target directory\n",
        "for file_name in os.listdir(source_dir):\n",
        "    # Construct full file path\n",
        "    file_path = os.path.join(source_dir, file_name)\n",
        "\n",
        "    # Check if it is a file and not a directory, then copy it\n",
        "    if os.path.isfile(file_path):\n",
        "        # Define the target file path\n",
        "        target_file_path = os.path.join(target_dir, file_name)\n",
        "\n",
        "        # Copy the file using shutil.copy2 to preserve metadata\n",
        "        shutil.copy2(file_path, target_file_path)\n",
        "        print(f'File copied: {file_name}')\n",
        "\n",
        "print(f'All files copied to %s.', target_dir)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1FHYXQv4x6WzvoFYcHkRzzw3I8e3mX7rc",
      "authorship_tag": "ABX9TyNsqHzv1izUQrLnqVcNKL5V",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}